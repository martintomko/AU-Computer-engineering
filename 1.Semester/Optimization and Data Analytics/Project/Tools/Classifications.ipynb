{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import add_dummy_feature # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.add_dummy_feature.html\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestC:\n",
    "    def __init__(self):\n",
    "        self.nc = NearestCentroid()\n",
    "        #self.centroids = []\n",
    "        \n",
    "    def fit(self, trainImgs, trainLbls):\n",
    "        self.nc.fit(trainImgs, trainLbls)\n",
    "        #self.centroids = self.nc.centroids_\n",
    "        return self\n",
    "    \n",
    "    def predict(self, testImgs):\n",
    "        return self.nc.predict(testImgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Subclass Centroid\n",
    "https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\n",
    "https://github.com/Woobs8/OPTI_Project/blob/master/classify.py\n",
    "(Inspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestSubclassC:\n",
    "    def __init__(self, subclassCount):\n",
    "        self.kmeans = KMeans(n_clusters=subclassCount)\n",
    "        self.subclass_centers = []\n",
    "        self.classes = []\n",
    "        self.subclassCount = subclassCount\n",
    "        \n",
    "    def fit(self, trainData, trainLbls):\n",
    "        #Create set from labels so we have every of them only once (Those are our classes)\n",
    "        self.classes = np.unique(trainLbls)\n",
    "        classesCount = len(self.classes)\n",
    "        #make empty array for groups\n",
    "        dataSamples, dataFeatures = trainData.shape\n",
    "        #make empty array for centers\n",
    "        self.subclass_centers = np.zeros((classesCount, self.subclassCount, dataFeatures))\n",
    "        group_trainData = []\n",
    "        index = 0\n",
    "        for classLbl in self.classes:\n",
    "            tmpData = []\n",
    "            for i, data in enumerate(trainData):\n",
    "                if trainLbls[i]==classLbl:\n",
    "                    tmpData.append(data)\n",
    "            group_trainData.append(tmpData)\n",
    "            \n",
    "            #Find subclasses\n",
    "            self.kmeans.fit(group_trainData[index])\n",
    "            #Get centers\n",
    "            self.subclass_centers[index] = self.kmeans.cluster_centers_\n",
    "            index = index+1\n",
    "        return self\n",
    "    \n",
    "    def predict(self, testData):\n",
    "        classCount = len(self.classes)\n",
    "        dataSamples, dataFeatures = testData.shape\n",
    "        distances = np.zeros((classCount, dataSamples))\n",
    "\n",
    "        # loop over classes and get distances to each subclass\n",
    "        for k in range(classCount):\n",
    "            classDistances = np.sqrt(((testData - (self.subclass_centers[k,:,:])[:,np.newaxis,:]) ** 2).sum(axis=2))\n",
    "            distances[k] = np.min(classDistances, axis=0)\n",
    "\n",
    "        return np.asarray(np.argmin(distances,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestN:\n",
    "    def __init__(self, neighbor_count, cpus):\n",
    "        self.nn = KNeighborsClassifier(neighbor_count, weights=\"distance\",  n_jobs=cpus)\n",
    "\n",
    "    def fit(self, trainData, trainLbls):\n",
    "        self.nn.fit(trainData, trainLbls)\n",
    "        return self\n",
    "\n",
    "    def predict(self, testData):\n",
    "        return self.nn.predict(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Perceptron\n",
    "http://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/ (Inspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self):\n",
    "         pass\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    " \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)       \n",
    " \n",
    "    def forward(self, data):\n",
    "        store = {}\n",
    "        X = data.T\n",
    "        \n",
    "        #Apply to hidden layer weights\n",
    "        Z = self.w1.dot(X) + self.b1\n",
    "        X = self.sigmoid(Z)\n",
    "        store[\"X1\"] = X\n",
    "        store[\"W1\"] = self.w1\n",
    "        store[\"Z1\"] = Z\n",
    "\n",
    "        #Apply output layer weights\n",
    "        Z = self.w2.dot(X) + self.b2\n",
    "        X = self.softmax(Z)\n",
    "        store[\"X2\"] = X\n",
    "        store[\"W2\"] = self.w2\n",
    "        store[\"Z2\"] = Z\n",
    "        return X, store\n",
    " \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    " \n",
    "    def backward(self, data, labels, store):\n",
    " \n",
    "        derivatives = {}\n",
    "        predicted = store[\"X2\"]\n",
    "        \n",
    "        #Calculate error\n",
    "        dZ = predicted - labels.T\n",
    "        \n",
    "        #Apply error to our w2 weight set\n",
    "        dAPrev = store[\"W2\"].T.dot(dZ)\n",
    "        derivatives[\"dW2\"] = dZ.dot(store[\"X1\"].T) / self.samples\n",
    "        derivatives[\"db2\"] = np.sum(dZ, axis=1, keepdims=True) / self.samples\n",
    " \n",
    "        #Apply error to w1 weights\n",
    "        dZ = dAPrev * self.sigmoid_derivative(store[\"Z1\"])\n",
    "        derivatives[\"dW1\"] = 1. / self.samples * dZ.dot(data)\n",
    "        derivatives[\"db1\"] = 1. / self.samples * np.sum(dZ, axis=1, keepdims=True)\n",
    "        return derivatives\n",
    " \n",
    "    def fit(self, data, labels, hiddenSize=50, learningRate=0.01, iterations=2500): \n",
    "        self.samples, self.features = data.shape\n",
    "        \n",
    "        #Define sizes of our layers\n",
    "        self.inputSize = self.features\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = labels.shape[1]\n",
    "            \n",
    "        ## initiaze random weight sets and empty bias sets\n",
    "        self.w1 = np.random.randn(self.hiddenSize, self.inputSize) / np.sqrt(self.inputSize)\n",
    "        self.b1 = np.zeros((self.hiddenSize, 1))\n",
    "        self.w2 = np.random.randn(self.outputSize, self.hiddenSize) / np.sqrt(self.hiddenSize)\n",
    "        self.b2 = np.zeros((self.outputSize, 1))\n",
    "            \n",
    " \n",
    "        for loop in range(iterations):\n",
    "            #Let neural network predict output\n",
    "            A, store = self.forward(data)\n",
    "            \n",
    "            #Calculate errors and get derivatives to adjust weights\n",
    "            derivatives = self.backward(data, labels, store)\n",
    "            \n",
    "            #apply derivatives, learning rate and bias to our weights datasets\n",
    "            self.w1 = self.w1 - learningRate * derivatives[\"dW1\"]\n",
    "            self.b1 = self.b1 - learningRate * derivatives[\"db1\"]\n",
    "            self.w2 = self.w2 - learningRate * derivatives[\"dW2\"]\n",
    "            self.b2 = self.b2 - learningRate * derivatives[\"db2\"]\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        A, cache = self.forward(data)\n",
    "        result = np.argmax(A, axis=0)\n",
    "        return result\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE Perceptron\n",
    "https://github.com/Woobs8/OPTI_Project/blob/master/classify.py [inspiration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    #epsilon: scaling of regularized pseudo-inverse matrix\n",
    "    def fit(self, train_data, train_lbls, epsilon):\n",
    "        classes = np.unique(train_lbls)\n",
    "        X = add_dummy_feature(train_data).transpose()\n",
    "        n_features, n_samples = X.shape\n",
    "        \n",
    "        # Calculate regularized pseudo-inverse of X\n",
    "        X_pinv = np.dot(np.linalg.inv(np.dot(X, X.transpose()) + epsilon * np.identity(n_features)), X)\n",
    "\n",
    "        B = np.where(train_lbls[np.newaxis, :] == classes[:, np.newaxis], 1, -1)\n",
    "\n",
    "        self.W = np.dot(B, X_pinv.transpose())\n",
    "        return self\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        X = add_dummy_feature(test_data).transpose()\n",
    "        decision = np.dot(self.W,X)\n",
    "        return np.argmax(decision,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
